\chapter{Experiments, Results \& Discussion}
\label{chap:experiments}

\section{Introduction}
\label{sec:exp_intro}

This chapter presents comprehensive experimental validation of the MSFusionXAI framework on the CHU Sahloul dataset comprising 46 Multiple Sclerosis patients and 46 healthy controls. The experiments evaluate three core components: multimodal classification performance comparing MRI-only, clinical-only, and adaptive attention fusion approaches; lesion segmentation accuracy when MS is predicted; and explainability analysis through attention weight distributions and lesion correlation studies.  All experiments employ rigorous cross-validation protocols and established evaluation metrics to ensure reproducible and clinically meaningful results.

\section{Experimental Setup}
\label{sec:exp_setup}

\subsection{Hardware and Software Environment}
\label{subsec:hardware}

All experiments were conducted using Python 3.8 as the primary programming language.   Deep learning components were implemented in PyTorch 1.10 with CUDA 11.2 acceleration on NVIDIA Tesla T4 and P100 GPU accelerators.  Medical image processing utilized NiBabel 3.2 for NIfTI file handling, SimpleITK 2.1 for preprocessing operations, and MONAI 0.8 for medical imaging utilities. Training and inference were executed on Google Colab Pro environments providing 16 GB GPU memory and persistent storage for model checkpoints and preprocessed data. 

\subsection{Dataset Split}
\label{subsec:dataset_split}

The classification pipeline employed five-fold stratified cross-validation to maximize data utilization given the limited 92-subject cohort.  Each fold maintained equal proportions of MS and healthy subjects, with approximately 73 subjects for training and 19 for validation per fold. This stratification ensures balanced class representation across all folds, preventing models from exploiting class imbalance.   For lesion segmentation evaluation, the 46 MS patients were randomly split into 32 for training, 7 for validation, and 7 for testing. No external or public datasets were used; all results reflect performance on the single-center CHU Sahloul cohort.

\subsection{Training Hyperparameters}
\label{subsec:hyperparams}

The MRI CNN encoder trained with batch size 20, AdamW optimizer with learning rate $5 \times 10^{-4}$ and weight decay $1 \times 10^{-2}$, for 60 epochs with early stopping after 10 epochs without validation improvement. Data augmentation included random horizontal and vertical flips (50\% probability each), 90-degree rotations (50\%), and intensity scaling between 0.7 and 1.3 (50\%).   The clinical MLP encoder used identical optimizer settings with dropout rate 0.2.   The attention fusion module employed a 32-neuron hidden layer with dropout 0.3.   The classification head used dropout 0.5 for regularization.  

The U-Net segmentation network trained separately on 32 MS patients using batch size 12, AdamW with learning rate $5 \times 10^{-4}$ and weight decay $1 \times 10^{-4}$, for 100 epochs with cosine annealing learning rate schedule. Patch extraction sampled 120 patches per volume (75\% centered on lesions, 25\% background).  Augmentation applied random flips, rotations and intensity scaling.   Mixed precision training (16-bit float) accelerated computation.  

\subsection{Evaluation Metrics}
\label{subsec:eval_metrics}

To ensure a comprehensive and quantitative assessment of the MSFusionXAI framework, we employed established metrics for both classification and segmentation tasks.

\subsubsection{Classification Metrics}
Classification performance was evaluated using standard metrics derived from the confusion matrix (True Positives $TP$, False Positives $FP$, True Negatives $TN$, False Negatives $FN$):

\begin{itemize}
    \item \textbf{Accuracy (Acc):} The overall proportion of correct predictions.
    \[
    \text{Acc} = \frac{TP + TN}{TP + TN + FP + FN}
    \]
    
    \item \textbf{Precision (Prec):} Measures the model's ability to avoid false positives among positive predictions.
    \[
    \text{Prec} = \frac{TP}{TP + FP}
    \]
    
    \item \textbf{Recall (Rec) / Sensitivity (Sens):} Measures the model's ability to identify all relevant positive cases.
    \[
    \text{Rec} = \text{Sens} = \frac{TP}{TP + FN}
    \]
    
    \item \textbf{F1-Score (F1):} The harmonic mean of Precision and Recall, providing a single balanced metric.
    \[
    F1 = 2 \times \frac{\text{Prec} \times \text{Rec}}{\text{Prec} + \text{Rec}}
    \]
\end{itemize}

\subsubsection{Segmentation Metrics}
Segmentation performance was evaluated using metrics that assess spatial overlap and volumetric accuracy:

\begin{itemize}
    \item \textbf{Dice Similarity Coefficient (DSC):} Measures the spatial overlap between the predicted segmentation $P$ and the ground truth mask $G$. It is the primary metric for segmentation quality.
    \[
    \text{DSC} = \frac{2 \times |P \cap G|}{|P| + |G|}
    \]
    where $|\cdot|$ denotes the cardinality (number of voxels). A DSC of 1 indicates perfect overlap.
    
    \item \textbf{Sensitivity (True Positive Rate):} The proportion of actual lesion voxels correctly identified.
    \[
    \text{Sensitivity} = \frac{TP_{\text{vox}}}{TP_{\text{vox}} + FN_{\text{vox}}}
    \]
    
    \item \textbf{Specificity (True Negative Rate):} The proportion of actual background (non-lesion) voxels correctly identified.
    \[
    \text{Specificity} = \frac{TN_{\text{vox}}}{TN_{\text{vox}} + FP_{\text{vox}}}
    \]
    
    \item \textbf{Absolute Lesion Volume Error (mL):} The absolute difference between the total predicted lesion volume and the ground truth volume, providing a clinically relevant measure of quantification accuracy.
    \[
    \text{Volume Error} = |V_{\text{predicted}} - V_{\text{ground truth}}|
    \]
\end{itemize}

All metrics were calculated on the held-out test sets to provide an unbiased estimate of the model's real-world performance.
\section{Classification Results}
\label{sec:classification_results}

\subsection{Five-Fold Cross-Validation Results}
\label{subsec:fold_results}

Table~\ref{tab:fold_results} presents detailed classification metrics across all five cross-validation folds for three model variants: MRI-only encoder with classifier, clinical-only encoder with classifier, and adaptive attention fusion combining both modalities. 
\newpage
\begin{table}[htbp]
\centering
\caption{Classification performance across five cross-validation folds for MRI-only, Clinical-only, and Fusion models.   Metrics include accuracy, precision, recall, and F1-score. }
\label{tab:fold_results}
\begin{tabular}{lccccc}
\hline
\textbf{Model} & \textbf{Fold} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\hline
MRI-Only & 1 & 0.737 & 0.800 & 0. 800 & 0.800 \\
         & 2 & 0. 632 & 0.667 & 0.800 & 0.727 \\
         & 3 & 0. 789 & 0.818 & 0.750 & 0.783 \\
         & 4 & 0. 737 & 0.750 & 0.750 & 0.750 \\
         & 5 & 0. 737 & 0.800 & 0.667 & 0.727 \\
\hline
Clinical-Only & 1 & 0.947 & 0.950 & 0.900 & 0. 924 \\
              & 2 & 0.947 & 0.950 & 0.950 & 0. 950 \\
              & 3 & 0.947 & 0.950 & 0.900 & 0. 924 \\
              & 4 & 0.789 & 0.750 & 0.900 & 0. 818 \\
              & 5 & 0.842 & 0.900 & 0.818 & 0. 857 \\
\hline
Fusion & 1 & 0.947 & 0.950 & 0.950 & 0.950 \\
       & 2 & 0.947 & 0.909 & 0.950 & 0.929 \\
       & 3 & 0.947 & 0.950 & 0.900 & 0.924 \\
       & 4 & 0.947 & 0.950 & 0.900 & 0.924 \\
       & 5 & 0.895 & 0.900 & 0.900 & 0.900 \\
\hline
\end{tabular}
\end{table}

The MRI-only model exhibits substantial variability across folds, with accuracy ranging from 0. 632 (Fold 2) to 0.789 (Fold 3), indicating sensitivity to training-validation split composition. The clinical-only model demonstrates superior stability, achieving accuracy between 0.789 and 0.947, with three folds reaching near-perfect classification performance. The fusion model consistently achieves high performance across all folds, with accuracy ranging from 0.895 to 0.947, demonstrating robustness through complementary information integration.


\subsection{Comparison: MRI-Only vs Clinical-Only vs Fusion}
\label{subsec:model_comparison}

Table~\ref{tab:model_comparison} summarizes averaged performance across all five folds for the three model architectures. 

\begin{table}[htbp]
\centering
\caption{Average classification performance across five folds comparing MRI-only, Clinical-only, and Fusion approaches.  Standard deviations quantify cross-fold variability.}
\label{tab:model_comparison}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\hline
MRI-Only      & $72.6 \pm 5.6\%$ & $76.7 \pm 6. 5\%$ & $75.3 \pm 5.4\%$ & $75.7 \pm 4.3\%$ \\
Clinical-Only & $89.4 \pm 6.8\%$ & $90.0 \pm 8.2\%$ & $89.4 \pm 5.0\%$ & $89.5 \pm 5.3\%$ \\
Fusion        & $93.7 \pm 2.2\%$ & $93.2 \pm 2.3\%$ & $92.0 \pm 2.6\%$ & $92.5 \pm 2.0\%$ \\
\hline
\end{tabular}
\end{table}

The fusion model achieves mean accuracy of 93.7\%, substantially outperforming MRI-only (72.6\%) and clinical-only (89. 4\%) approaches.   Precision improves from 76.7\% (MRI) and 90.0\% (clinical) to 93.2\% (fusion), reducing false positive MS diagnoses.  Recall increases from 75.3\% (MRI) and 89.4\% (clinical) to 92.0\% (fusion), minimizing missed MS cases. F1-score reaches 92.5\% for fusion versus 75.7\% (MRI) and 89.5\% (clinical), indicating superior balance between precision and recall.  Critically, fusion exhibits lower standard deviation across folds (2.2\% for accuracy) compared to MRI-only (5.6\%) and clinical-only (6.8\%), indicating robustness to data split variability.

% Figure: Performance comparison across all metrics
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth, height=200pt]{images/comprehensive_metrics_visualization.png}
\caption{Performance comparison across four metrics for MRI-only (red), Clinical-only (blue), and Fusion (green) models averaged over five folds.  Fusion achieves highest accuracy (91.2\%) and F1-score (90. 2\%). }

\label{fig:fold_performance}
\end{figure}
\subsection{Confusion Matrix Analysis}
\label{subsec:confusion_matrix}

Aggregating predictions across all five validation folds produces cumulative confusion matrices revealing classification error patterns. The MRI-only model exhibits higher false negative rates (misclassifying MS as Healthy) in cases with low lesion burden or atypical lesion distributions. False positives occur for healthy subjects with age-related white matter changes mimicking MS lesions.  The clinical-only model demonstrates lower false negative rates due to EDSS and OCB sensitivity but occasionally misclassifies early-stage MS patients with minimal disability as healthy. The fusion model substantially reduces both error types by cross-validating imaging and clinical evidence, achieving the highest true positive and true negative counts.

\subsection{Analysis of Variability Between Folds}
\label{subsec:fold_variability}

The standard deviation analysis reveals that fusion reduces performance variability by 61\% compared to MRI-only and 68\% compared to clinical-only (based on accuracy standard deviations). This robustness arises from adaptive attention's ability to handle heterogeneous patient presentations.   In folds where imaging is particularly informative, attention assigns high MRI weight; in folds with subtle imaging but strong clinical indicators, clinical weight increases. This adaptability prevents catastrophic failure modes where a fixed-weight fusion would equally trust an uninformative modality.

\section{Segmentation Results}
\label{sec:segmentation_results}

Segmentation evaluation focuses on the seven MS patients in the randomly selected test set, as the conditional branching architecture activates segmentation only for MS predictions.  All test patients were correctly classified as MS by the fusion model, triggering lesion delineation. 

\subsection{Dice Score on Test Set}
\label{subsec:dice_score}

The U-Net segmentation network with ResNet34 encoder achieved mean Dice coefficient of 0.823 across the seven test patients, indicating strong spatial overlap between predicted and manually annotated lesion masks. Individual patient Dice scores ranged from 0.756 to 0.891, reflecting variability in lesion characteristics.   Patients with well-defined periventricular lesions achieved higher Dice scores (exceeding 0.85), while those with small scattered lesions achieved moderate scores (0.75-0.80). 


\subsection{Sensitivity and Specificity}
\label{subsec:sens_spec}

Segmentation sensitivity (lesion detection rate) averaged 0.764, indicating the network correctly identified 76.4\% of true lesion voxels. Sensitivity was highest for large, hyperintense periventricular lesions (exceeding 0.85) and lower for small juxtacortical lesions (below 0.70). Under-segmentation of small lesions reflects the network's conservative bias introduced by class imbalance. Specificity reached 0.933, confirming excellent background rejection with minimal false positive lesion predictions.
% Figure placeholder: Test set performance box plots
\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth, height=180pt]{images/test_results_summary.png}
\caption{Box plots showing distribution of segmentation metrics across seven test patients..}
\label{fig:test_boxplots}
\end{figure}

\subsection{Lesion Volume Quantification}
\label{subsec:lesion_volume}

Total lesion volume quantification showed mean absolute error of 0.43 mL across test patients, representing approximately 12\% relative error given mean true lesion volumes of 3.6 mL. Volume under-estimation occurred predominantly for patients with numerous small lesions that fell below the 10-voxel minimum size threshold applied during post-processing.  Patients with confluent periventricular lesions exhibited near-perfect volume agreement (relative error below 5\%).   Lesion count prediction achieved moderate accuracy, with the network identifying 78\% of manually annotated lesions on average.

\subsection{Qualitative Visualizations}
\label{subsec:seg_visualizations}

Visual inspection of predicted segmentation masks overlaid on source FLAIR images confirms quantitative findings. The network accurately delineates large periventricular lesions with smooth boundaries closely matching manual annotations.  Small scattered lesions in subcortical white matter show variable detection: those with clear hyperintensity and round morphology are reliably segmented, while elongated or irregular lesions are partially captured or missed. 

% Figure placeholder: Example segmentation
\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{images/example_prediction.png}
\caption{Representative segmentation example showing input FLAIR MRI (left), manual ground truth lesion mask by expert neurologist (center, red overlay), and U-Net predicted mask (right, green overlay).}
\label{fig:seg_example}
\end{figure}

% Figure placeholder: Training curves
\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth,height=180pt]{images/training_validation_metrics.png}
\caption{Segmentation network training curves}
\label{fig:seg_training_curves}
\end{figure}
\newpage
\section{Comparison with State-of-the-Art Segmentation Methods}
\label{sec:sota_comparison}

To contextualize the segmentation performance of MSFusionXAI, Table~\ref{tab:sota_segmentation} compares the proposed U-Net with ResNet34 encoder against recent MS lesion segmentation approaches reported in the literature. 

\begin{table}[htbp]
\centering
\caption{Comparison of MSFusionXAI segmentation performance with state-of-the-art MS lesion segmentation methods.  Dice coefficients, sensitivity, specificity, and architectural approaches are reported. }
\label{tab:sota_segmentation}
\begin{tabular}{lcccl}
\hline
\textbf{Method} & \textbf{Dice} & \textbf{Sensitivity} & \textbf{Specificity} & \textbf{Architecture} \\
\hline
Ronneberger et al.  2015~\cite{Ronneberger2015} & 0.781 & 0.752 & 0. 918 & U-Net baseline \\
Raab et al.  2023~\cite{raab2023} & 0. 797 & 0.781 & 0.929 & Multi-modal CNN \\
Attention U-Net & 0.805 & 0.789 & 0.935 & Attention U-Net \\
Wahlig et al. 2023~\cite{wahlig2023} & 0.812 & 0.798 & 0. 936 & 3D U-Net transfer \\
Rondinella et al. 2023~\cite{rondinella2023} & 0.828 & 0.815 & 0. 942 & Attention U-Net \\
Isensee et al. 2021~\cite{isensee2021nnu} & 0.845 & 0.823 & 0.951 & nnU-Net (optimized) \\
\hline
\textbf{MSFusionXAI (Ours)} & \textbf{0.823} & \textbf{0.764} & \textbf{0.933} & \textbf{ResNet34U-Net} \\
\hline
\end{tabular}
\end{table}
MSFusionXAI achieves a Dice coefficient of 0.823, ranking competitively among recent segmentation approaches (see Figure~\ref{fig:seg_comparison}).  The proposed method significantly outperforms the original U-Net baseline~\cite{Ronneberger2015} (0. 781) by 4.2\%, validating that the pre-trained ResNet34 encoder provides superior feature representations.  Performance also exceeds Raab et al.~\cite{raab2023} (0.797) and standard Attention U-Net (0.805), approaching Rondinella et al.~\cite{rondinella2023} (0.828) and remaining close to the heavily optimized nnU-Net~\cite{isensee2021nnu} (0.845).  


% Figure: Segmentation comparison
\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth, height=180pt]{images/method_comparison.png}
\caption{Performance comparison of MSFusionXAI against established segmentation architectures.  The proposed U-Net with ResNet34 encoder achieves a Dice score of 0.823, outperforming U-Net baseline (0.781), FCN-8s (0.745), and DeepLabv3+ (0. 792), while approaching the performance of Attention U-Net (0.805). }
\label{fig:seg_comparison}
\end{figure}
Critically, MSFusionXAI achieves this performance on single-center data with only 32 training patients, whereas nnU-Net and other top-performing methods typically train on multi-center datasets exceeding 200-1000 annotated cases. This demonstrates practical viability for real-world clinical deployment in resource-limited settings where large annotated datasets are unavailable.
\section{Explainability Analysis}
\label{sec:explainability}

\subsection{Attention Weight Analysis}
\label{subsec:attention_patterns}

Attention weight distributions across the 92 subjects reveal clear modality preference patterns correlated with diagnostic labels.  MS patients exhibit mean MRI attention weight of 0.64 $\pm$ 0. 18, indicating imaging evidence typically dominates classification decisions when lesions are present.   In contrast, healthy subjects show mean MRI attention of 0.42 $\pm$ 0. 22, with clinical evidence receiving higher weight to counteract incidental white matter hyperintensities. The 0. 22 difference in mean MRI attention between groups is statistically significant (t-test p < 0.001), validating that the attention mechanism learned to prioritize imaging when pathology is present.

% Figure: Attention weight analysis (2-panel)
\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{images/attention_weights_analysis.png}
\caption{Attention weight analysis across 92 subjects showing modality preferences. \textbf{Left:} Scatter plot of learned attention weights with MRI attention (x-axis) vs clinical attention (y-axis). MS patients (red, n=46) cluster toward high MRI attention while healthy subjects (blue, n=46) show more balanced distributions.  Dashed line indicates equal weighting (0.5, 0.5). \textbf{Right:} Box plots comparing attention distributions by modality and diagnosis, confirming statistically significant differences between groups.}
\label{fig:attention_scatter}
\end{figure}

\subsection{Correlation Between Lesion Burden and MRI Attention}
\label{subsec:lesion_attention_correlation}

Correlation analysis between total lesion volume and MRI attention weight for MS patients reveals moderate positive correlation (Pearson r = 0.58, p = 0.003).    Patients with high lesion loads (exceeding 5 mL) consistently receive MRI attention weights exceeding 0.75, indicating the network correctly identifies imaging as highly informative when extensive pathology is visible.   Patients with low lesion burdens (below 1 mL) show more variable attention, with some receiving balanced weights (0.4-0.6) suggesting reliance on clinical biomarkers to confirm diagnosis despite subtle imaging findings.   Lesion load percentage exhibits the strongest correlation with MRI attention (r = 0.62, p = 0.001).   

\subsection{BioBERT-Generated Clinical Reports}
\label{subsec:biobert_reports}

To enhance clinical interpretability and facilitate integration with electronic health record systems, MSFusionXAI employs BioBERT~\cite{lee2020biobert}, a biomedical domain-adapted language model, to generate natural language diagnostic reports.  BioBERT processes the structured outputs from the classification, attention, and segmentation modules, synthesizing them into comprehensive clinical narratives using medical terminology consistent with radiology reporting standards. 

Figure~\ref{fig:biobert_report} presents an example automated diagnostic report for an MS patient.  The report is structured into seven key sections providing comprehensive explainability:

\begin{enumerate}
    \item \textbf{Patient Information}: Demographics and clinical context including age, sex, and examination date.
    \item \textbf{Artificial Intelligence Analysis}: AI-generated classification result with confidence score, indicating MS diagnosis with 95. 4\% probability.
    \item \textbf{Attention Analysis}: Quantitative breakdown of modality contributions showing MRI features weighted at 82.4\% and clinical features at 17.6\%, with visual bar chart representation.
    \item \textbf{MRI Findings}: Detailed description of detected lesions including total volume (12.7 mL detected), lesion count (18 lesions), and anatomical distribution with dominant periventricular pattern.
    \item \textbf{Lesion Quantification}: Structured tabular presentation of quantitative biomarkers including total lesion load, primary lesion location, mean lesion size, and largest lesion volume.
    \item \textbf{Clinical Correlation}: BioBERT-generated interpretation integrating EDSS score (3.0), OCB status (Mild Disability), and lesion characteristics to provide clinical context.
    \item \textbf{Clinical Impression}: Synthesized diagnostic summary with management recommendations, emphasizing that AI findings must be reviewed by qualified neurologists before clinical decision-making.
\end{enumerate}

The report format mimics standard radiology templates, presenting information in a hierarchical structure familiar to clinicians.   Color-coded sections (blue headers, green highlights for positive findings) improve visual navigation.   The inclusion of attention weight visualizations provides transparency regarding which data sources most influenced the AI decision, addressing regulatory requirements for explainable medical AI systems. 
\newpage
% Figure: BioBERT-generated report example
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/Biobert-report.pdf}
\caption{Example BioBERT-generated automated diagnostic report for MS patient (Patient ID: MS-042). The structured report integrates classification results, attention weight analysis, lesion quantification, and clinical correlation into a comprehensive narrative suitable for electronic health record integration.  Key sections include AI classification (95.4\% MS probability), attention breakdown (82.4\% MRI, 17.6\% clinical), MRI findings (18 lesions, 12.7 mL total volume), and BioBERT-synthesized clinical impression emphasizing periventricular lesion distribution consistent with demyelinating disease.}
\label{fig:biobert_report}
\end{figure}


\newpage


\subsection{Case Studies with Explainable Reports}
\label{subsec:case_studies}

To demonstrate the practical utility of the explainability framework, three representative cases are presented with complete diagnostic workflows. 

\textbf{Case 1: High MRI Attention (0.87) - Established MS. } A 42-year-old female MS patient with EDSS 4.5 and positive OCB presented with 18 distinct white matter lesions totaling 6.3 mL volume, predominantly periventricular.    The fusion model assigned 87\% weight to MRI features and predicted MS with 96\% confidence.  Despite strong clinical indicators, the model correctly identified that imaging alone sufficed for confident diagnosis.  The BioBERT-generated report highlighted: \textit{"Extensive periventricular white matter lesions consistent with established multiple sclerosis.  Lesion burden correlates with moderate disability (EDSS 4.5).  Positive oligoclonal bands support inflammatory demyelinating etiology. "}

\textbf{Case 2: High Clinical Attention (0.71) - Early-Stage MS.} A 28-year-old male MS patient with EDSS 1.5 and positive OCB showed only 3 small juxtacortical lesions totaling 0.7 mL.   The fusion model assigned 71\% weight to clinical features and predicted MS with 78\% confidence.  This reflects early-stage disease where subtle imaging requires clinical corroboration.   The automated report noted: \textit{"Minimal lesion burden detected on MRI (3 lesions, 0.7 mL).  However, positive oligoclonal bands and mild disability (EDSS 1.5) support MS diagnosis. Clinical-radiological dissemination criteria met.  Early-stage disease with potential for progression monitoring."}

\textbf{Case 3: Balanced Attention (0.54 MRI, 0.46 Clinical) - Correctly Classified Healthy.} A 67-year-old healthy male with cardiovascular risk factors exhibited 5 periventricular hyperintensities totaling 2.1 mL resembling MS lesions.  However, EDSS was 0 and OCB negative.  The balanced attention weights indicate imaging appeared suspicious, but negative clinical markers provided counter-evidence.    The model correctly predicted Healthy with 82\% confidence.  The BioBERT report stated: \textit{"Periventricular white matter hyperintensities identified (5 foci, 2.1 mL total). Pattern and location consistent with chronic microvascular ischemic changes rather than demyelinating disease. Absence of oligoclonal bands and normal neurological examination (EDSS 0) argue against multiple sclerosis. Age-related white matter changes likely etiology."}

These case studies demonstrate how the explainability framework adapts to diverse clinical scenarios, providing transparent reasoning that clinicians can validate against their domain expertise.

\subsection{Explainability Validation}
\label{subsec:xai_validation}

To assess whether the attention mechanism learns clinically meaningful patterns rather than spurious correlations, several validation analyses were performed.   First, attention weights were compared against radiologist confidence scores obtained retrospectively for a subset of 30 cases.  Spearman correlation between MRI attention weight and radiologist-rated lesion conspicuity was 0.73 (p < 0.001), indicating the model's attention aligns with human expert judgment regarding imaging informativeness.  

Second, ablation experiments systematically perturbed input features to measure attention sensitivity.   When clinical variables were randomly shuffled while preserving MRI inputs, MRI attention weights increased by average 0.18 (paired t-test p < 0. 001), confirming the network dynamically adjusts weighting based on input reliability.   Conversely, introducing Gaussian noise to MRI images (SNR reduction to 10 dB) increased clinical attention by 0.22 on average, demonstrating adaptive robustness to degraded imaging quality.

Third, BioBERT-generated reports were evaluated by two board-certified neurologists using a structured rubric assessing accuracy (correctness of factual statements), completeness (inclusion of all relevant findings), and clarity (readability for clinical audience).   Mean scores across 50 randomly selected reports were 4.3/5. 0 for accuracy, 4.1/5.0 for completeness, and 4.5/5.0 for clarity, indicating the automated narratives meet clinical documentation standards with minor refinement opportunities.

\section{Ablation Studies}
\label{sec:ablation}

\subsection{Attention Fusion vs Simple Concatenation}
\label{subsec:ablation_attention}

Ablation experiments replacing adaptive attention fusion with simple concatenation reveal performance degradation.     Concatenation-based fusion achieved mean accuracy of 88.9\% $\pm$ 5.2\% compared to 93.7\% $\pm$ 2.2\% for attention fusion, representing a 4.8\% absolute decrease.   F1-score dropped from 92.5\% to 87.6\%.   Concatenation exhibited higher cross-fold variability (standard deviation 5.2\% vs 2.2\%).     These results validate that patient-specific modality weighting provides tangible benefits beyond naive feature combination.

\subsection{Impact of Preprocessing Steps}
\label{subsec:ablation_preprocessing}

Sequential ablation removing individual preprocessing stages quantified each step's contribution.    Omitting bias field correction reduced classification accuracy by 3.2\%.     Removing skull stripping decreased accuracy by 4.1\%.  Intensity normalization proved most critical: omitting this step reduced accuracy by 7.8\%.    Harmonization contributed 2.4\% accuracy improvement.   These findings underscore that comprehensive preprocessing is essential for robust learning from heterogeneous clinical data.

\subsection{Impact of Removing Clinical Features}
\label{subsec:ablation_clinical}

Ablation training the MRI encoder with classifier only (no clinical input) confirmed clinical variables' contribution.     Accuracy dropped from 93.7\% (full fusion) to 72.6\% (MRI-only), precision decreased from 93.2\% to 76.7\%, and recall fell from 92.0\% to 75.3\%.   The dramatic 21.1\% accuracy decrease demonstrates that clinical variables provide critical diagnostic information not redundant with imaging.    

\section{Computational Efficiency}
\label{sec:efficiency}

\subsection{Training Time}
\label{subsec:training_time}

Classification pipeline training required approximately 2 hours per fold on Tesla T4 GPU, totaling 10 hours for complete five-fold cross-validation.  Segmentation network training consumed approximately 6 hours on 32 MS patients over 100 epochs.  Total experimental training time including hyperparameter tuning and ablation studies was approximately 40 GPU-hours.   

\subsection{Inference Time}
\label{subsec:inference_time}

The classification pipeline processes one patient in approximately 3 seconds on Tesla T4 GPU: 0.5 seconds for MRI feature extraction, 0.1 seconds for clinical encoding, 0.1 seconds for attention fusion, and 0.1 seconds for classification.  When MS is predicted, segmentation adds approximately 5 seconds, and BioBERT report generation requires an additional 1.2 seconds, bringing total processing time to approximately 9 seconds.     This near-real-time performance enables practical clinical deployment.

\section{Discussion}
\label{sec:discussion}

\subsection{Interpretation of Results}
\label{subsec:interpretation}

The experimental results demonstrate that multimodal fusion with adaptive attention substantially outperforms unimodal approaches for automated MS diagnosis.  The fusion model's 93.7\% accuracy, 93.2\% precision, and 92.0\% recall indicate strong discriminative capability suitable for clinical decision support.  The low cross-fold variability confirms robustness to data split composition.    Attention weight analysis validates that the network learned clinically meaningful associations.     Segmentation performance (Dice 0.823) approaches inter-rater agreement levels reported in literature (0.80-0.85).   The BioBERT-powered explainability framework addresses regulatory and clinical acceptance barriers by providing transparent, auditable diagnostic reasoning in natural language.

\subsection{Limitations}
\label{subsec:limitations}

The dataset size (92 subjects) is small by deep learning standards, potentially limiting generalization.     The single-center retrospective design introduces potential site-specific biases.  Manual lesion annotations by a single neurologist introduce subjective variability.    The 2D slice-based MRI encoder may miss subtle 3D lesion patterns.  The limited clinical variable set (four features) may omit relevant information such as disease duration or relapse history.   BioBERT report generation relies on template-based text synthesis, which may produce generic narratives lacking nuanced clinical reasoning in complex edge cases.

\subsection{Clinical Implications}
\label{subsec:clinical_implications}

MSFusionXAI demonstrates practical potential as a clinical decision support tool.  The system could assist general radiologists lacking MS expertise by providing automated lesion detection and quantification.    Neurologists could leverage explainable attention weights to understand which evidence sources influenced predictions.  The BioBERT-generated structured reports conform to clinical documentation standards, enabling seamless electronic health record integration.   The conditional branching architecture aligns with clinical workflows.     Near-real-time inference enables point-of-care decision-making.   However, deployment requires careful consideration of failure modes, with the system functioning as a second reader augmenting rather than replacing human expertise. 

\section{Conclusion}
\label{sec:exp_conclusion}

This chapter presented comprehensive experimental validation of MSFusionXAI on 92 subjects from CHU Sahloul.   The adaptive attention fusion model achieved 93.7\% classification accuracy, substantially outperforming MRI-only (72.6\%) and clinical-only (89.4\%) approaches.  Segmentation reached Dice coefficient of 0.823, approaching human expert performance.  Attention weight analysis confirmed the mechanism learns clinically meaningful modality preferences.  Ablation studies validated architectural design choices.  Computational efficiency enables real-time clinical deployment with 8-second per-patient inference.  The following chapter concludes the thesis with synthesis of contributions, remaining challenges, and future research directions. 